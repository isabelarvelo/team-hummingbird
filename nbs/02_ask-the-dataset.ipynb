{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook demonstrates the backend logic of a Data Analysis Chatbot. Built with OpenAI's GPT model, this interactive tool allows users to input their API key, the paths to their data, and their questions. The model processes this information to provide insightful answers to the user's queries about their dataset. The AI-powered assistant is designed to understand and analyze data uploaded in CSV format, delivering its analysis in a natural, conversational manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp AskTheDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import os\n",
    "import getpass\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CSVFileManager:\n",
    "    \"\"\"This class is responsible for handling CSV file operations. \n",
    "    It can read one or more CSV files, either from file paths or file-like objects, and concatenate them into a\n",
    "    single pandas DataFrame.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.data_frame = None\n",
    "\n",
    "    def load_data(self, files):\n",
    "        \n",
    "        if not files:\n",
    "            raise ValueError(\"No files provided.\")\n",
    "\n",
    "        data_frames = []\n",
    "        for file in files:\n",
    "            # Check if the file is a string (path) or a file-like object\n",
    "            if isinstance(file, str):\n",
    "                df = pd.read_csv(file)\n",
    "            else:  # Assuming file-like object\n",
    "                df = pd.read_csv(file)\n",
    "            data_frames.append(df)\n",
    "\n",
    "        self.data_frame = pd.concat(data_frames, ignore_index=True)\n",
    "        return self.data_frame\n",
    "\n",
    "class GPTQuestionsHandler:\n",
    "    \"\"\"This class interfaces with OpenAI's GPT model. \n",
    "    It sends user queries about the dataset to the GPT model and retrieves responses.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "\n",
    "        # Initializes the OpenAI API client\n",
    "\n",
    "        self.api_key = api_key\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "        self.client = OpenAI()\n",
    "\n",
    "    def ask_gpt(self, question, data):\n",
    "        \"\"\"\n",
    "        Sends a question and data to the OpenAI API and retrieves the response\n",
    "        Arguments: The user's question about the data\n",
    "        Data is input to the model in string format\n",
    "        Model returns the response from the API\n",
    "        \"\"\"\n",
    "\n",
    "        system_message = (\"You are a helpful assistant skilled at data science and data analysis. \"\n",
    "                          \"You are an expert at reading files, interpreting them and also writing python codes. \"\n",
    "                          \"Here is the data you need to work with:\\n\" + data)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ])\n",
    "        return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  How many rows are in the data? Also give me a python code to find this out for any dataset \n",
      "\n",
      "Answer: The data contains 33 rows. Here's a Python code snippet to find the number of rows in any dataset using pandas:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Load the dataset\n",
      "data = pd.read_csv(\"your_dataset.csv\")  # Replace \"your_dataset.csv\" with the actual file name\n",
      "\n",
      "# Get the number of rows\n",
      "num_rows = data.shape[0]\n",
      "print(\"Number of rows:\", num_rows)\n",
      "```\n",
      "\n",
      "Replace \"your_dataset.csv\" with the actual file name of your dataset. This code will load the dataset into a pandas DataFrame and then print the number of rows in the dataset. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "def main():\n",
    "    file_paths_input = input(\"Enter CSV file paths, separated by a comma: \")\n",
    "    file_paths = [path.strip() for path in file_paths_input.split(',')]\n",
    "\n",
    "    data_manager = CSVFileManager()\n",
    "    data_frame = data_manager.load_data(file_paths)\n",
    "\n",
    "\n",
    "    api_key = getpass.getpass(\"Enter OpenAI API Key: \")\n",
    "    ai_handler = GPTQuestionsHandler(api_key)\n",
    "\n",
    "    user_question = input(\"Please enter your question about the data: \")\n",
    "    answer = ai_handler.ask_gpt(user_question, data_frame.to_string(index=False))\n",
    "\n",
    "    print(\"Question: \", user_question,\"\\n\")\n",
    "    print(\"Answer:\", answer, \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentary on code functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of GPT Model Responses with examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Can you provide examples of sentences labeled as 'PRS' \n",
      "\n",
      "Answer: Sure! Here are some examples of sentences labeled as 'PRS' from the provided data:\n",
      "\n",
      "1. Great example, 'run' is a verb because it is an action. \n",
      "2. Great job, that sentence definitely needed an exclamation mark!\n",
      "3. She reads a book.\n",
      "4. She runs quickly.\n",
      "5. That's a perfect example! Declarative sentences end with a period.\n",
      "6. Great job! That is indeed an interrogative sentence.\n",
      "7. Wonderful! That sentence is showing strong emotion, so it is an exclamatory sentence.\n",
      "8. Well done, everyone! You have learned a lot today.\n",
      "9. Well done, everyone! You have learned a lot today.\n",
      "10. I am looking forward to reading your stories.\n",
      "11. Well done, everyone! You have learned a lot today.\n",
      "12. I am looking forward to reading your paragraphs.\n",
      "13. Have a great day, and remember to keep practicing your English! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "def main():\n",
    "    file_paths_input = '../data/009-1.csv, ../data/009-2.csv'\n",
    "    file_paths = [path.strip() for path in file_paths_input.split(',')]\n",
    "\n",
    "    data_manager = CSVFileManager()\n",
    "    data_frame = data_manager.load_data(file_paths)\n",
    "\n",
    "\n",
    "    api_key = getpass.getpass(\"Enter OpenAI API Key: \")\n",
    "    ai_handler = GPTQuestionsHandler(api_key)\n",
    "\n",
    "    user_question = \"Can you provide examples of sentences labeled as 'PRS'\"\n",
    "    answer = ai_handler.ask_gpt(user_question, data_frame.to_string(index=False))\n",
    "\n",
    "    print(\"Question: \", user_question,\"\\n\")\n",
    "    print(\"Answer:\", answer, \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What are the unique labels in the Label column? \n",
      "\n",
      "Answer: The unique labels in the \"Label\" column are: PRS, OTR, and NaN. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "def main():\n",
    "    file_paths_input = '../data/009-1.csv'\n",
    "    file_paths = [path.strip() for path in file_paths_input.split(',')]\n",
    "\n",
    "    data_manager = CSVFileManager()\n",
    "    data_frame = data_manager.load_data(file_paths)\n",
    "\n",
    "\n",
    "    api_key = getpass.getpass(\"Enter OpenAI API Key: \")\n",
    "    ai_handler = GPTQuestionsHandler(api_key)\n",
    "\n",
    "    user_question = \"What are the unique labels in the Label column?\"\n",
    "    answer = ai_handler.ask_gpt(user_question, data_frame.to_string(index=False))\n",
    "\n",
    "    print(\"Question: \", user_question,\"\\n\")\n",
    "    print(\"Answer:\", answer, \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Response to \"How many unique labels are there in the Label column?\"\n",
    "The model correctly identifies three unique labels: 'PRS', 'OTR', and 'NaN'.\n",
    "'NaN' is mentioned as a label, which indicates the model understands 'NaN' (commonly used to represent missing data in pandas DataFrames) as a category. This shows an understanding of data handling conventions.\n",
    "The response indicates the model's ability to interpret and categorize data based on provided information.\n",
    "\n",
    "2. Response to \"Can you provide examples of sentences labeled as 'PRS'\"\n",
    "The model provided a list of sentences which are assumed to be labeled as 'PRS'.\n",
    "The model's response demonstrates its ability to extract and present data based on specified criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above two examples, we can see that overall, the model shows a strong understanding of the context and structure of the data. It can differentiate between labels and provide relevant examples. The provided examples seem relevant and accurately categorized based on the 'PRS' label. This indicates the model's effectiveness in filtering and presenting data as per user queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model can provide information based on the data it's been given in the prompt, and can provide the python code to perform analyses, it can't run codes and provide visualizations.\n",
    "\n",
    "\n",
    "The model's responses to queries about the dataset showcase its capabilities in data analysis, context understanding, and relevant information extraction. The model effectively interprets and categorizes data, providing coherent and contextually appropriate responses. These capabilities make it a valuable tool for gaining insights from structured data, such as CSV files, although it's important to remember its limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
